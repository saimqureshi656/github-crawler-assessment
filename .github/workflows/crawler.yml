# ============================================================
# GitHub Actions Workflow - GitHub Crawler
# ============================================================

name: GitHub Crawler

# ------------------------------------------------------------
# WHEN TO RUN
# ------------------------------------------------------------
on:
  push:
    branches: [ main ]          # Run when code is pushed to main branch
  workflow_dispatch:            # Allow manual trigger from GitHub UI
  schedule:
    - cron: '0 0 * * *'        # Run daily at midnight UTC (optional)

# ------------------------------------------------------------
# MAIN JOB
# ------------------------------------------------------------
jobs:
  crawl:
    runs-on: ubuntu-latest      # Free Linux VM

    # --------------------------------------------------------
    # POSTGRES SERVICE CONTAINER
    # --------------------------------------------------------
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: github_crawler
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    # --------------------------------------------------------
    # STEPS
    # --------------------------------------------------------
    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Install Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      # Step 3: Install dependencies
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      # Step 4: Wait for PostgreSQL to be fully ready
      - name: Wait for PostgreSQL
        run: |
          for i in {1..30}; do
            pg_isready -h localhost -p 5432 -U postgres && break
            echo "Waiting for postgres..."
            sleep 2
          done
          echo "✅ PostgreSQL is ready!"

      # Step 5: Create database tables
      - name: Setup database schema
        run: |
          python -c "
          from src.db.connection import DatabaseManager
          db = DatabaseManager('postgresql://postgres:postgres@localhost:5432/github_crawler')
          db.connect()
          db.setup_schema('src/db/schema.sql')
          db.close()
          print('✅ Schema created successfully')
          "

      # Step 6: Run crawler
      - name: Crawl GitHub repositories
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/github_crawler
        run: |
          python src/main.py

      # Step 7: Export results to CSV (FIXED VERSION)
      - name: Export database results
        run: |
          # Install PostgreSQL client tools
          sudo apt-get update && sudo apt-get install -y postgresql-client
          
          # Use COPY ... TO STDOUT instead of \COPY to avoid Bash parsing errors
          PGPASSWORD=postgres psql -h localhost -U postgres -d github_crawler -c "
          COPY (
            SELECT 
              r.id, 
              r.full_name, 
              r.owner,
              r.name,
              rs.star_count,
              rs.recorded_at
            FROM repositories r
            JOIN repository_stars rs ON r.id = rs.repository_id
            ORDER BY rs.star_count DESC
          ) TO STDOUT WITH CSV HEADER
          " > results.csv

          echo '✅ CSV export complete'

      # Step 8: Upload results artifact
      - name: Upload results artifact
        uses: actions/upload-artifact@v4
        with:
          name: crawler-results-${{ github.run_number }}
          path: results.csv
          retention-days: 30
